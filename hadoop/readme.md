# Word Count - Hadoop Map Reduce Example – How it works?
![shuffle](https://user-images.githubusercontent.com/84274712/140618617-378c69d3-826b-4b23-a3ef-f076821bef20.PNG)
Explanation: Taking in STDIN and STDOUT ( standard input and standard output ) helps passing data between the Reduce and Map code. sys.stdout to print the output and sys stdin to read the input is used in Python.

Splitting: The parameter of splitter can be anything. By comma, space, by a new line or a semicolon.

Mapping: This is done as explained below.

Shuffle / Intermediate splitting: The process is usually parallel on cluster keys. The output of the map gets into the Reducer phase and all the similar keys of data are aligned in a cluster.

Reduce: This is done as explained below. Final result – All the data is clustered or combined to show the together form of a result.

The input given is converted into the string. Then it toknises them into words as if it need to break them. The mapper will append a single number or digit to each word and mapper outputs are shown above. Once we get the outputs as key-value pairs, once we pass the offset address as input to the mapper, the output of the value would be key-value pairs.

The output is getting into the sorting and shuffling phase. When we sort based on keys, all the keys will come to once a particular place. Sorting on the keys and shuffling the keys is done. A single word will go to a single reducer. Input to the reducer is key-value pairs. Once we pass outputs to reducer as input, the reducer will sum up all the values to keys.

That is, it groups up all the similar keys and output would be the concatenated key-value pair. The reducer will pick the result from the temp path and it will arrive at the final result.  When we execute map-reduce, the input and output should be created in HDFS. Which is why import a lot of files that will help do the word count. We use something called a job client to do configuration. Extends configure and implements the tools.

## Hadoop WordCount operation occurs in 3 stages –

1) Mapper Phase
2) Shuffle Phase
3) Reducer Phase

## 1. Map() Function
Create and process the import data. Takes in data, converts it into a set of other data where the breakdown of individual elements into tuples is done. No API contract requiring a certain number of outputs.

## 2. Reduce() Function
Mappers’ output is passed into the reduction. Processes the data into something usable. Every single mapper is passed into the reduced function. The new output values are saved into HDFS. A concept called streaming is used in writing a code for word count in Python using MapReduce. Let’s look at the mapper Python code and a Reducer Python code and how to execute that using a streaming jar file. The API has a technical name for this task which is shuffle and sort phase.

### Hadoop WordCount Example- Mapper Phase Execution
The text from the input text file is tokenized into words to form a key value pair with all the words present in the input text file. The key is the word from the input file and value is ‘1’.

For instance if you consider the sentence “the quick brown fox jumped over lazy dogs quick dogs jumped jumped”. The mapper phase in the WordCount example will split the string into individual tokens i.e. words. In this case, the entire sentence will be split into 12 tokens (one for each word) with a value 1 as shown below –

#### Key-Value pairs from Hadoop Map Phase Execution-
the quick brown fox jumped over lazy dogs quick dogs jumped jumped
- (the,1)
- (quick,1) 
- (brown,1) 
- (fox,1) 
- (jumped,1) 
- (over,1) 
- (lazy,1) 
- (dogs,1) 
- (quick,1)
- (dogs,1)
- (jumped,1)
- (jumped,1)

#### Hadoop WordCount Example- Shuffle Phase Execution

After the map phase execution is completed successfully, shuffle phase is executed automatically wherein the key-value pairs generated in the map phase are taken as input and then sorted in alphabetical order. After the shuffle phase is executed from the WordCount example code, the output will look like this -
- (brown,1)
- (dogs,1) 
- (dogs,1)
- (fox,1)
- (jumped,1)
- (jumped,1)
- (jumpe,1)
- (lazy,1)
- (over,1)
- (quick,1)
- (the,1)

#### Hadoop WordCount Example- Reducer Phase Execution
In the reduce phase, all the keys are grouped together and the values for similar keys are added up to find the occurrences for a particular word. It is like an aggregation phase for the keys generated by the map phase. The reducer phase takes the output of shuffle phase as input and then reduces the key-value pairs to unique keys with values added up. In our example “the quick brown fox jumped over lazy dogs quick dogs jumped jumped” is the only word that appears twice in the sentence. After the execution of the reduce phase of MapReduce WordCount example program, appears as a key only once but with a count of 2 as shown below -
- (brown,1)
- (dogs,2)
- (fox,1)
- (jumped,3)
- (lazy,1)
- (over,1)
- (quick,1)
- (the,1)
